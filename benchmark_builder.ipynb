{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook builds a long context benchmark, as discussed by Federico, Arjun,\n",
    "Harm, and Leandro. Unlike typical Code LLM benchmarks, this is a test\n",
    "generation benchmark: we prompt the model with the implementation of a Python\n",
    "function (and its docstring), and ask for a test suite. The result is scored in\n",
    "two steps: if any test in the test suite fails, the score is zero. Otherwise,\n",
    "we the tests are scored based on their coverage of the funciton's\n",
    "implementation. To make the problem harder, we add several other functions to\n",
    "the prompt to serve as distractors. There are enough distractors to exercise\n",
    "models with very long context lengths (up to 128K tokens). We use two datasets:\n",
    "HumanEval and MultiPL-T. Both have several Python functions. The HumanEval\n",
    "functions should be decontaminated before training: their docstrings should not\n",
    "appear in the training data. The MultiPL-T functions are functions extracted\n",
    "from the Stack v1.2. Thus they are very likely to appear in models' training\n",
    "data, but they are merely distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import os\n",
    "from typing import List\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# In case you're in an environment where you really want this to be set.\n",
    "print(os.getenv(\"HF_DATASETS_CACHE\"))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# This is likely an overestimate. But, it should be close enough and we don't need\n",
    "# to be exact.\n",
    "CHARS_PER_TOKEN = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openai_humaneval (/home/arjun/.cache/huggingface/datasets/openai_humaneval/openai_humaneval/1.0.0/2955cebd73602e828fa8c0a424c594e5fab4ec863b316ca98f3d8fdb6a626e75)\n",
      "Found cached dataset parquet (/home/arjun/.cache/huggingface/datasets/nuprl___parquet/nuprl--stack-dedup-python-testgen-starcoder-filter-inferred-v2-8a147987b4874669/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "humaneval = datasets.load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "# The MultiPL-T dataset is currently private, but will be public soon.\n",
    "multiplt = datasets.load_dataset(\"nuprl/stack-dedup-python-testgen-starcoder-filter-inferred-v2\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors(approximate_token_count: int) -> List[str]:\n",
    "    result = []\n",
    "    result_chars = 0\n",
    "    target_chars = int(approximate_token_count * CHARS_PER_TOKEN)\n",
    "    while result_chars < target_chars:\n",
    "        fn = random.choice(multiplt)[\"content\"]\n",
    "        result.append(fn)\n",
    "        result_chars += len(fn)\n",
    "    return result\n",
    "\n",
    "def build_prompt(\n",
    "        approximate_token_count: int,\n",
    "        humaneval_problem_index: int,\n",
    "        insert_where: str):\n",
    "    distractors = get_distractors(approximate_token_count)\n",
    "    target_problem = humaneval[humaneval_problem_index]\n",
    "    target_function = target_problem[\"prompt\"] + target_problem[\"canonical_solution\"]\n",
    "    if insert_where == \"first half\":\n",
    "        insert_index = random.randint(0, len(distractors) // 2)    \n",
    "    elif insert_where == \"second half\":\n",
    "        insert_index = random.randint(len(distractors) // 2, len(distractors))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown insert_where: {insert_where}\")\n",
    "    distractors.insert(insert_index, target_function)\n",
    "    return { \n",
    "        \"prompt\": \"\\n\\n\".join(distractors),\n",
    "        \"target_function\": target_function,\n",
    "        \"humaneval_task_id\": target_problem[\"task_id\"],\n",
    "        \"task_id\": f\"LongBench_{target_problem['task_id']}_{approximate_token_count}_{insert_where}\",\n",
    "        \"approx_token_count\": approximate_token_count,\n",
    "        \"target_function_name\": target_problem[\"entry_point\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prompts\n",
    "\n",
    "Some examples of prompts that we can construct.\n",
    "\n",
    "With 0 as the number of target tokens, we get no distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(0, 53, \"first half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 400 target tokens, we get 1-2 distractors and the `where` argument starts to\n",
    "make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_cli_fname(lon, lat, scenario=0):\n",
      "    \"\"\"Get the climate file name for the given lon, lat, and scenario\"\"\"\n",
      "    # The trouble here is relying on rounding is problematic, so we just\n",
      "    # truncate\n",
      "    lon = round(lon, 2)\n",
      "    lat = round(lat, 2)\n",
      "    return \"/i/%s/cli/%03ix%03i/%06.2fx%06.2f.cli\" % (\n",
      "        scenario,\n",
      "        0 - lon,\n",
      "        lat,\n",
      "        0 - lon,\n",
      "        lat,\n",
      "    )\n",
      "\n",
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n",
      "\n",
      "def remove_single_characters(text):\n",
      "    \"\"\"\n",
      "    Remove any remaining single-character words\n",
      "    :text: string\n",
      "    :return: string\n",
      "    \"\"\"\n",
      "    return ' '.join([word for word in text.split() if len(word) > 1])\n",
      "\n",
      "def get_thread_id_from_suggestion_id(suggestion_id):\n",
      "    \"\"\"Gets the thread_id from the suggestion_id.\n",
      "\n",
      "    Args:\n",
      "        suggestion_id: str. The ID of the suggestion.\n",
      "\n",
      "    Returns:\n",
      "        str. The thread ID linked to the suggestion.\n",
      "    \"\"\"\n",
      "    return suggestion_id[suggestion_id.find('.') + 1:]\n",
      "\n",
      "def from_bamstats(stats, value):\n",
      "    \"\"\"Return percentage that a part represents of a total.\"\"\"\n",
      "    if value == \"mean target coverage\":\n",
      "        return stats.get(\"summary\", {}).get(\"mean coverage\")\n",
      "    elif value == \"total target size\":\n",
      "        return stats.get(\"summary\", {}).get(\"total target size\")\n",
      "    elif value == \"total reads\":\n",
      "        return stats[\"bamstats\"][\"sequences\"] // 2\n",
      "    elif value == \"percent duplicates\":\n",
      "        if stats[\"bamstats\"][\"sequences\"]:\n",
      "            return 100.0 * stats[\"bamstats\"][\"reads duplicated\"] / stats[\"bamstats\"][\"sequences\"]\n",
      "        else:\n",
      "            return 0.0\n",
      "    else:\n",
      "        return \"INVALID\"\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(400, 53, \"first half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def clean_fields(fields):\n",
      "    \"\"\"\n",
      "    Clean and return a ``fields`` list of Deb822Field.\n",
      "    \"\"\"\n",
      "    for hf in (fields or []):\n",
      "        hf.rstrip()\n",
      "    return fields\n",
      "\n",
      "def Compact2D(m):\n",
      "    \"\"\"\n",
      "    Decodes the 64 bit morton code into a 32 bit number in the 2D space using\n",
      "    a divide and conquer approach for separating the bits. \n",
      "    1 bit is not used because the integers are not unsigned\n",
      "    \n",
      "    Args:\n",
      "        n (int): a 64 bit morton code\n",
      "        \n",
      "    Returns:\n",
      "        int: a dimension in 2D space\n",
      "        \n",
      "    Raises:\n",
      "        Exception: ERROR: Morton code is always positive\n",
      "    \"\"\"\n",
      "    if m < 0:\n",
      "        raise Exception(\"\"\"ERROR: Morton code is always positive\"\"\")\n",
      "    m &= 0x5555555555555555\n",
      "    m = (m ^ (m >> 1))  & 0x3333333333333333\n",
      "    m = (m ^ (m >> 2))  & 0x0f0f0f0f0f0f0f0f\n",
      "    m = (m ^ (m >> 4))  & 0x00ff00ff00ff00ff\n",
      "    m = (m ^ (m >> 8))  & 0x0000ffff0000ffff\n",
      "    m = (m ^ (m >> 16)) & 0x00000000ffffffff\n",
      "    return m\n",
      "\n",
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n",
      "\n",
      "def dot(u, v):\n",
      "    \"\"\"\n",
      "    Take dot product between two arrays.\n",
      "    \"\"\"\n",
      "    return sum(u[i]*v[i] for i in range(len(u)))\n",
      "\n",
      "def speed_convert(size):\n",
      "    \"\"\"\n",
      "    Hi human, you can't read bytes?\n",
      "    \"\"\"\n",
      "    power = 2**10\n",
      "    zero = 0\n",
      "    units = {0: \"\", 1: \"Kb/s\", 2: \"Mb/s\", 3: \"Gb/s\", 4: \"Tb/s\"}\n",
      "    while size > power:\n",
      "        size /= power\n",
      "        zero += 1\n",
      "    return f\"{round(size, 2)} {units[zero]}\"\n",
      "\n",
      "def parse_info_date_str(info_date_str):\n",
      "    \"\"\"Returns an info_date string modified in such a way that Elasticsearch would not attempt to interpret it as a date.\n",
      "    Currently there are several different formats of info_date used.\n",
      "    If no modification is applied Elasticseach will interpret part of the values as a string and another part as a date\n",
      "    which causes a value error and should be avoided.\n",
      "\n",
      "    :param info_date_str:\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    return 'str:' + info_date_str\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(400, 53, \"second half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Benchmark\n",
    "\n",
    "There are a number of trivial problems in HumanEval, such as #53 shown above.\n",
    "We want a subset of problems that have a range of difficulties. The following\n",
    "ten problems have varying difficulty in several programming languages\n",
    "and were picked by Francesca Lucchetti for MultiPL-T.\n",
    "\n",
    "- HumanEval_100_make_a_pile\n",
    "- HumanEval_13_greatest_common_divisor\n",
    "- HumanEval_152_compare\n",
    "- HumanEval_157_right_angle_triangle\n",
    "- HumanEval_27_flip_case\n",
    "- HumanEval_40_triples_sum_to_zero\n",
    "- HumanEval_55_fib\n",
    "- HumanEval_66_digitSum\n",
    "- HumanEval_72_will_it_fly\n",
    "- HumanEval_74_total_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'target_function', 'humaneval_task_id', 'task_id', 'approx_token_count', 'target_function_name'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPROXIMATE_TOKEN_COUNTS = [0, 8_000, 64_000, 128_000]\n",
    "HUMANEVAL_PROBLEM_INDICES = [100, 13, 152, 157, 27, 40, 55, 66, 72, 74]\n",
    "INSERT_WHERES = [ \"first half\", \"second half\"]\n",
    "\n",
    "benchmark = datasets.Dataset.from_list(\n",
    "    [build_prompt(*x) for x in itertools.product(APPROXIMATE_TOKEN_COUNTS, HUMANEVAL_PROBLEM_INDICES, INSERT_WHERES)])\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.66ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14995862"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.to_json(\"benchmark.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\ndef make_a_pile(n):\\n    \"\"\"\\n    Given a positive integer n, you have to make a pile of n levels of stones.\\n    The first level has n stones.\\n    The number of stones in the next level is:\\n        - the next odd number if n is odd.\\n        - the next even number if n is even.\\n    Return the number of stones in each level in a list, where element at index\\n    i represents the number of stones in the level (i+1).\\n\\n    Examples:\\n    >>> make_a_pile(3)\\n    [3, 5, 7]\\n    \"\"\"\\n    return [n + 2*i for i in range(n)]\\n',\n",
       " 'target_function': '\\ndef make_a_pile(n):\\n    \"\"\"\\n    Given a positive integer n, you have to make a pile of n levels of stones.\\n    The first level has n stones.\\n    The number of stones in the next level is:\\n        - the next odd number if n is odd.\\n        - the next even number if n is even.\\n    Return the number of stones in each level in a list, where element at index\\n    i represents the number of stones in the level (i+1).\\n\\n    Examples:\\n    >>> make_a_pile(3)\\n    [3, 5, 7]\\n    \"\"\"\\n    return [n + 2*i for i in range(n)]\\n',\n",
       " 'humaneval_task_id': 'HumanEval/100',\n",
       " 'task_id': 'LongBench_HumanEval/100_0_first half',\n",
       " 'approx_token_count': 0,\n",
       " 'target_function_name': 'make_a_pile'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
