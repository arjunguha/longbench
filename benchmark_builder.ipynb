{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook builds a long context benchmark, as discussed by Federico, Arjun,\n",
    "Harm, and Leandro. Unlike typical Code LLM benchmarks, this is a test\n",
    "generation benchmark: we prompt the model with the implementation of a Python\n",
    "function (and its docstring), and ask for a test suite. The result is scored in\n",
    "two steps: if any test in the test suite fails, the score is zero. Otherwise,\n",
    "we the tests are scored based on their coverage of the funciton's\n",
    "implementation. To make the problem harder, we add several other functions to\n",
    "the prompt to serve as distractors. There are enough distractors to exercise\n",
    "models with very long context lengths (up to 128K tokens). We use two datasets:\n",
    "HumanEval and MultiPL-T. Both have several Python functions. The HumanEval\n",
    "functions should be decontaminated before training: their docstrings should not\n",
    "appear in the training data. The MultiPL-T functions are functions extracted\n",
    "from the Stack v1.2. Thus they are very likely to appear in models' training\n",
    "data, but they are merely distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elleven/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import os\n",
    "from typing import List\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# In case you're in an environment where you really want this to be set.\n",
    "print(os.getenv(\"HF_DATASETS_CACHE\"))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# This is likely an overestimate. But, it should be close enough and we don't need\n",
    "# to be exact.\n",
    "CHARS_PER_TOKEN = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openai_humaneval (/home/elleven/.cache/huggingface/datasets/openai_humaneval/openai_humaneval/1.0.0/2955cebd73602e828fa8c0a424c594e5fab4ec863b316ca98f3d8fdb6a626e75)\n",
      "Found cached dataset parquet (/home/elleven/.cache/huggingface/datasets/nuprl___parquet/nuprl--stack-dedup-python-testgen-starcoder-filter-inferred-v2-8a147987b4874669/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading readme: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 620/620 [00:00<00:00, 8.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/elleven/.cache/huggingface/datasets/nuprl___parquet/nuprl--humaneval-py-mutants-b940dfa114eb395a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                                                                                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 245k/245k [00:00<00:00, 5.05MB/s]\u001b[A\n",
      "Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2364.32it/s]\n",
      "                                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/elleven/.cache/huggingface/datasets/nuprl___parquet/nuprl--humaneval-py-mutants-b940dfa114eb395a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "humaneval = datasets.load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "# The MultiPL-T dataset is currently private, but will be public soon.\n",
    "multiplt = datasets.load_dataset(\"nuprl/stack-dedup-python-testgen-starcoder-filter-inferred-v2\", split=\"train\")\n",
    "# mutant generated dataset\n",
    "mutant_ds = datasets.load_dataset(\"nuprl/humaneval-py-mutants\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors(approximate_token_count: int) -> List[str]:\n",
    "    result = []\n",
    "    result_chars = 0\n",
    "    target_chars = int(approximate_token_count * CHARS_PER_TOKEN)\n",
    "    while result_chars < target_chars:\n",
    "        fn = random.choice(multiplt)[\"content\"]\n",
    "        result.append(fn)\n",
    "        result_chars += len(fn)\n",
    "    return result\n",
    "\n",
    "def find_mutant_with_idx(humaneval_problem_index: int):\n",
    "    for ex in mutant_ds:\n",
    "        if f\"_{humaneval_problem_index}_\" in ex[\"name\"]:\n",
    "            return ex\n",
    "    return None\n",
    "\n",
    "def build_prompt(\n",
    "        approximate_token_count: int,\n",
    "        humaneval_problem_index: int,\n",
    "        insert_where: str):\n",
    "    distractors = get_distractors(approximate_token_count)\n",
    "    target_problem = humaneval[humaneval_problem_index]\n",
    "    target_function = target_problem[\"prompt\"] + target_problem[\"canonical_solution\"]\n",
    "    if insert_where == \"first half\":\n",
    "        insert_index = random.randint(0, len(distractors) // 2)    \n",
    "    elif insert_where == \"second half\":\n",
    "        insert_index = random.randint(len(distractors) // 2, len(distractors))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown insert_where: {insert_where}\")\n",
    "    distractors.insert(insert_index, target_function)\n",
    "    mutants = find_mutant_with_idx(humaneval_problem_index)[\"mutants\"]\n",
    "    mutants = mutants if mutants is not None else []\n",
    "    return { \n",
    "        \"prompt\": \"\\n\\n\".join(distractors),\n",
    "        \"target_function\": target_function,\n",
    "        \"humaneval_task_id\": target_problem[\"task_id\"],\n",
    "        \"task_id\": f\"LongBench_{target_problem['task_id']}_{approximate_token_count}_{insert_where}\",\n",
    "        \"approx_token_count\": approximate_token_count,\n",
    "        \"mutants\": mutants,\n",
    "        \"target_function_name\": target_problem[\"entry_point\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prompts\n",
    "\n",
    "Some examples of prompts that we can construct.\n",
    "\n",
    "With 0 as the number of target tokens, we get no distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(0, 53, \"first half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 400 target tokens, we get 1-2 distractors and the `where` argument starts to\n",
    "make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def speed_convert(size):\n",
      "    \"\"\"\n",
      "    Hi human, you can't read bytes?\n",
      "    \"\"\"\n",
      "    power = 2**10\n",
      "    zero = 0\n",
      "    units = {0: \"\", 1: \"Kb/s\", 2: \"Mb/s\", 3: \"Gb/s\", 4: \"Tb/s\"}\n",
      "    while size > power:\n",
      "        size /= power\n",
      "        zero += 1\n",
      "    return f\"{round(size, 2)} {units[zero]}\"\n",
      "\n",
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n",
      "\n",
      "def parse_info_date_str(info_date_str):\n",
      "    \"\"\"Returns an info_date string modified in such a way that Elasticsearch would not attempt to interpret it as a date.\n",
      "    Currently there are several different formats of info_date used.\n",
      "    If no modification is applied Elasticseach will interpret part of the values as a string and another part as a date\n",
      "    which causes a value error and should be avoided.\n",
      "\n",
      "    :param info_date_str:\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    return 'str:' + info_date_str\n",
      "\n",
      "def handle_prices(data: dict) -> dict:\n",
      "    \"\"\"Set price according to rate.\"\"\"\n",
      "    for reservation in data['Reservation']:\n",
      "        if reservation['Tarif'] == \"Plein tarif\":\n",
      "            price = 10.00\n",
      "        elif reservation['Tarif'] == \"Tarif reduit\":\n",
      "            price = 8.00\n",
      "        elif reservation['Tarif'] == \"Senior\":\n",
      "            price = 7.00\n",
      "        elif reservation['Tarif'] == \"Tarif etudiant\":\n",
      "            price = 7.00\n",
      "        else:\n",
      "            price = None\n",
      "        reservation['prix'] = price\n",
      "    return data\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(400, 53, \"first half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _LJ_rminepsilon_to_ab(coeffs):\n",
      "    \"\"\"\n",
      "    Convert rmin/epsilon representation to AB representation of the LJ\n",
      "    potential\n",
      "    \"\"\"\n",
      "    A = coeffs['epsilon'] * coeffs['Rmin']**12.0\n",
      "    B = 2 * coeffs['epsilon'] * coeffs['Rmin']**6.0\n",
      "    return {\"A\": A, \"B\": B}\n",
      "\n",
      "def check_if_odd(num):\n",
      "    \"\"\"\n",
      "    Checks if number is odd\n",
      "\n",
      "    Args:\n",
      "        num :\n",
      "\n",
      "\n",
      "    (Generated by docly)\n",
      "    \"\"\"\n",
      "    return True if num % 2 != 0 else False\n",
      "\n",
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n",
      "\n",
      "def declare(var_name, var_type): \n",
      "\t\"\"\"Creates an SMTLIB declaration formatted string\n",
      "\n",
      "\tParameters\n",
      "\t\t----------\n",
      "\t\tvar_name: string\n",
      "\t\t\tThe name of the variable\n",
      "\t\tvar_type: \n",
      "\t\t\tThe type of the variable (Int, Bool, etc.)\n",
      "\t\"\"\"\n",
      "\treturn \"(declare-fun \" + var_name + \" () \" + var_type + \")\\n\"\n",
      "\n",
      "def dict_scale(d, scl):\n",
      "    \"\"\"scales all values in dict and returns a new dict\"\"\"\n",
      "    return dict([(k, v * scl) for k, v in d.items()])\n",
      "\n",
      "def dasherize(word):\n",
      "    \"\"\"Replace underscores with dashes in the string.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> dasherize(\"lower_case\")\n",
      "        \"lower-case\"\n",
      "\n",
      "    \"\"\"\n",
      "    return word.replace(\"_\", \"-\")\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(400, 53, \"second half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Benchmark\n",
    "\n",
    "There are a number of trivial problems in HumanEval, such as #53 shown above.\n",
    "We want a subset of problems that have a range of difficulties. The following\n",
    "ten problems have varying difficulty in several programming languages\n",
    "and were picked by Francesca Lucchetti for MultiPL-T.\n",
    "\n",
    "- HumanEval_100_make_a_pile\n",
    "- HumanEval_13_greatest_common_divisor\n",
    "- HumanEval_152_compare\n",
    "- HumanEval_157_right_angle_triangle\n",
    "- HumanEval_27_flip_case\n",
    "- HumanEval_40_triples_sum_to_zero\n",
    "- HumanEval_55_fib\n",
    "- HumanEval_66_digitSum\n",
    "- HumanEval_72_will_it_fly\n",
    "- HumanEval_74_total_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'target_function', 'humaneval_task_id', 'task_id', 'approx_token_count', 'mutants', 'target_function_name'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPROXIMATE_TOKEN_COUNTS = [0, 8_000, 64_000, 128_000]\n",
    "HUMANEVAL_PROBLEM_INDICES = [100, 13, 152, 157, 27, 40, 55, 66, 72, 74]\n",
    "INSERT_WHERES = [ \"first half\", \"second half\"]\n",
    "\n",
    "benchmark = datasets.Dataset.from_list(\n",
    "    [build_prompt(*x) for x in itertools.product(APPROXIMATE_TOKEN_COUNTS, HUMANEVAL_PROBLEM_INDICES, INSERT_WHERES)])\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 12.38ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10746864"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.to_json(\"benchmark.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\ndef make_a_pile(n):\\n    \"\"\"\\n    Given a positive integer n, you have to make a pile of n levels of stones.\\n    The first level has n stones.\\n    The number of stones in the next level is:\\n        - the next odd number if n is odd.\\n        - the next even number if n is even.\\n    Return the number of stones in each level in a list, where element at index\\n    i represents the number of stones in the level (i+1).\\n\\n    Examples:\\n    >>> make_a_pile(3)\\n    [3, 5, 7]\\n    \"\"\"\\n    return [n + 2*i for i in range(n)]\\n',\n",
       " 'target_function': '\\ndef make_a_pile(n):\\n    \"\"\"\\n    Given a positive integer n, you have to make a pile of n levels of stones.\\n    The first level has n stones.\\n    The number of stones in the next level is:\\n        - the next odd number if n is odd.\\n        - the next even number if n is even.\\n    Return the number of stones in each level in a list, where element at index\\n    i represents the number of stones in the level (i+1).\\n\\n    Examples:\\n    >>> make_a_pile(3)\\n    [3, 5, 7]\\n    \"\"\"\\n    return [n + 2*i for i in range(n)]\\n',\n",
       " 'humaneval_task_id': 'HumanEval/100',\n",
       " 'task_id': 'LongBench_HumanEval/100_0_first half',\n",
       " 'approx_token_count': 0,\n",
       " 'target_function_name': 'make_a_pile'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
