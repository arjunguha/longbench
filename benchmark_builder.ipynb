{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook builds a long context benchmark, as discussed by Federico, Arjun,\n",
    "Harm, and Leandro. Unlike typical Code LLM benchmarks, this is a test\n",
    "generation benchmark: we prompt the model with the implementation of a Python\n",
    "function (and its docstring), and ask for a test suite. The result is scored in\n",
    "two steps: if any test in the test suite fails, the score is zero. Otherwise,\n",
    "we the tests are scored based on their coverage of the funciton's\n",
    "implementation. To make the problem harder, we add several other functions to\n",
    "the prompt to serve as distractors. There are enough distractors to exercise\n",
    "models with very long context lengths (up to 128K tokens). We use two datasets:\n",
    "HumanEval and MultiPL-T. Both have several Python functions. The HumanEval\n",
    "functions should be decontaminated before training: their docstrings should not\n",
    "appear in the training data. The MultiPL-T functions are functions extracted\n",
    "from the Stack v1.2. Thus they are very likely to appear in models' training\n",
    "data, but they are merely distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import os\n",
    "from typing import List\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# In case you're in an environment where you really want this to be set.\n",
    "print(os.getenv(\"HF_DATASETS_CACHE\"))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# This is likely an overestimate. But, it should be close enough and we don't need\n",
    "# to be exact.\n",
    "CHARS_PER_TOKEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openai_humaneval (/home/arjun/.cache/huggingface/datasets/openai_humaneval/openai_humaneval/1.0.0/2955cebd73602e828fa8c0a424c594e5fab4ec863b316ca98f3d8fdb6a626e75)\n",
      "Found cached dataset parquet (/home/arjun/.cache/huggingface/datasets/nuprl___parquet/nuprl--stack-dedup-python-testgen-starcoder-filter-inferred-v2-8a147987b4874669/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "humaneval = datasets.load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "# The MultiPL-T dataset is currently private, but will be public soon.\n",
    "multiplt = datasets.load_dataset(\"nuprl/stack-dedup-python-testgen-starcoder-filter-inferred-v2\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors(approximate_token_count: int) -> List[str]:\n",
    "    result = []\n",
    "    result_chars = 0\n",
    "    target_chars = int(approximate_token_count * CHARS_PER_TOKEN)\n",
    "    while result_chars < target_chars:\n",
    "        fn = random.choice(multiplt)[\"content\"]\n",
    "        result.append(fn)\n",
    "        result_chars += len(fn)\n",
    "    return result\n",
    "\n",
    "def build_prompt(\n",
    "        approximate_token_count: int,\n",
    "        humaneval_problem_index: int,\n",
    "        insert_where: str):\n",
    "    distractors = get_distractors(approximate_token_count)\n",
    "    target_problem = humaneval[humaneval_problem_index]\n",
    "    target_function = target_problem[\"prompt\"] + target_problem[\"canonical_solution\"]\n",
    "    if insert_where == \"first half\":\n",
    "        insert_index = random.randint(0, len(distractors) // 2)    \n",
    "    elif insert_where == \"second half\":\n",
    "        insert_index = random.randint(len(distractors) // 2, len(distractors))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown insert_where: {insert_where}\")\n",
    "    distractors.insert(insert_index, target_function)\n",
    "    return { \n",
    "        \"prompt\": \"\\n\\n\".join(distractors),\n",
    "        \"target_function\": target_function,\n",
    "        \"humaneval_task_id\": target_problem[\"task_id\"],\n",
    "        \"task_id\": f\"LongBench_{target_problem['task_id']}_{approximate_token_count}_{insert_where}\",\n",
    "        \"approx_token_count\": approximate_token_count,\n",
    "        \"target_function_name\": target_problem[\"entry_point\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prompts\n",
    "\n",
    "Some examples of prompts that we can construct.\n",
    "\n",
    "With 0 as the number of target tokens, we get no distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(0, 53, \"first half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 400 target tokens, we get 1-2 distractors and the `where` argument starts to\n",
    "make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def erathostenes_sieve(n: int) -> dict:\n",
      "    \"\"\"\n",
      "    1.6\n",
      "    Returns a dictionary containing prime numbers\n",
      "    It is not recommended to use a list (1) because of\n",
      "    reallocation and (2) because of index handling : the prime\n",
      "    numbers start at 2 and the list, 0; and because some numbers\n",
      "    are removed, the index changes too\n",
      "    \"\"\"\n",
      "    if n <= 0: raise ValueError(\"Argument can't be <= 0\")\n",
      "\n",
      "    primes = {n:None for n in range(2, n + 1)}\n",
      "\n",
      "    # We must convert it as a list, because the size changes\n",
      "    for prime in list(primes.keys()):\n",
      "        for number in list(primes.keys()):\n",
      "            # It is a composit number if it is a multiple of the prime\n",
      "            if prime != number and number%prime == 0:\n",
      "                del primes[number]\n",
      "\n",
      "    return primes\n",
      "\n",
      "def kewley_agn_oi(log_oi_ha):\n",
      "    \"\"\"Seyfert/LINER classification line for log([OI]/Ha).\"\"\"\n",
      "    return 1.18 * log_oi_ha + 1.30\n",
      "\n",
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n",
      "\n",
      "def atoi(text):\n",
      "    \"\"\"\n",
      "    Checks if the file names contain numbers.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    text\n",
      "        This parameter could be a str or int.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    flow  :  int, str\n",
      "    \"\"\"\n",
      "    flow = int(text) if text.isdigit() else text\n",
      "    return flow\n",
      "\n",
      "def get_total_time_in_sec(last_user_stats):\n",
      "    \"\"\"Calculates the total time you listen to music\"\"\"\n",
      "    total_time_sec = 0\n",
      "    for song in last_user_stats:\n",
      "        try:\n",
      "            total_time_sec += int(song['play_count']) * (int(song['duration_millis']) / 1000)\n",
      "        except:\n",
      "            continue\n",
      "    return total_time_sec\n",
      "\n",
      "def encode_varint(number: int) -> bytes:\n",
      "    \"\"\"\n",
      "    Encode varint into bytes\n",
      "    \"\"\"\n",
      "    # Shift to int64\n",
      "    number = number << 1\n",
      "    buf = b\"\"\n",
      "    while True:\n",
      "        towrite = number & 0x7F\n",
      "        number >>= 7\n",
      "        if number:\n",
      "            buf += bytes((towrite | 0x80,))\n",
      "        else:\n",
      "            buf += bytes((towrite,))\n",
      "            break\n",
      "    return buf\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(400, 53, \"first half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def seperate_kw(uns_kw):\n",
      "    \"\"\"Separate the keywords and return a list.\"\"\"\n",
      "    sep_kw = []\n",
      "\n",
      "    # Check if -> is present in the name\n",
      "    if '->' not in uns_kw:\n",
      "        sep_kw.append(uns_kw)\n",
      "    else:\n",
      "        while '->' in uns_kw:\n",
      "            pos = uns_kw.find(\"->\")\n",
      "            sep_kw.append(uns_kw[:pos])\n",
      "            uns_kw = uns_kw[pos + 2:]\n",
      "\n",
      "        sep_kw.append(uns_kw)\n",
      "    return sep_kw\n",
      "\n",
      "def _LJ_rminepsilon_to_ab(coeffs):\n",
      "    \"\"\"\n",
      "    Convert rmin/epsilon representation to AB representation of the LJ\n",
      "    potential\n",
      "    \"\"\"\n",
      "    A = coeffs['epsilon'] * coeffs['Rmin']**12.0\n",
      "    B = 2 * coeffs['epsilon'] * coeffs['Rmin']**6.0\n",
      "    return {\"A\": A, \"B\": B}\n",
      "\n",
      "def check_if_odd(num):\n",
      "    \"\"\"\n",
      "    Checks if number is odd\n",
      "\n",
      "    Args:\n",
      "        num :\n",
      "\n",
      "\n",
      "    (Generated by docly)\n",
      "    \"\"\"\n",
      "    return True if num % 2 != 0 else False\n",
      "\n",
      "def declare(var_name, var_type): \n",
      "\t\"\"\"Creates an SMTLIB declaration formatted string\n",
      "\n",
      "\tParameters\n",
      "\t\t----------\n",
      "\t\tvar_name: string\n",
      "\t\t\tThe name of the variable\n",
      "\t\tvar_type: \n",
      "\t\t\tThe type of the variable (Int, Bool, etc.)\n",
      "\t\"\"\"\n",
      "\treturn \"(declare-fun \" + var_name + \" () \" + var_type + \")\\n\"\n",
      "\n",
      "def dict_scale(d, scl):\n",
      "    \"\"\"scales all values in dict and returns a new dict\"\"\"\n",
      "    return dict([(k, v * scl) for k, v in d.items()])\n",
      "\n",
      "def dasherize(word):\n",
      "    \"\"\"Replace underscores with dashes in the string.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> dasherize(\"lower_case\")\n",
      "        \"lower-case\"\n",
      "\n",
      "    \"\"\"\n",
      "    return word.replace(\"_\", \"-\")\n",
      "\n",
      "def calc_dDdc_fn(c, dc, D_fn):\n",
      "    \"\"\"\n",
      "    Computes dD/dc given a functional form to estimate D(c).\n",
      "    \"\"\"\n",
      "    # computes diffusivity at given concentration and one step size away [m^2/s]\n",
      "    D1 = D_fn(c)\n",
      "    D2 = D_fn(c + dc)\n",
      "    # computes dD/dc using forward difference formula [m^2/s / kg/m^3]\n",
      "    dDdc = (D2 - D1) / dc\n",
      "\n",
      "    return dDdc\n",
      "\n",
      "\n",
      "\n",
      "def add(x: int, y: int):\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "    return x + y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(400, 53, \"second half\")[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Benchmark\n",
    "\n",
    "There are a number of trivial problems in HumanEval, such as #53 shown above.\n",
    "We want a subset of problems that have a range of difficulties. The following\n",
    "ten problems have varying difficulty in several programming languages\n",
    "and were picked by Francesca Lucchetti for MultiPL-T.\n",
    "\n",
    "- HumanEval_100_make_a_pile\n",
    "- HumanEval_13_greatest_common_divisor\n",
    "- HumanEval_152_compare\n",
    "- HumanEval_157_right_angle_triangle\n",
    "- HumanEval_27_flip_case\n",
    "- HumanEval_40_triples_sum_to_zero\n",
    "- HumanEval_55_fib\n",
    "- HumanEval_66_digitSum\n",
    "- HumanEval_72_will_it_fly\n",
    "- HumanEval_74_total_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'target_function', 'humaneval_task_id', 'task_id', 'approx_token_count', 'target_function_name'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPROXIMATE_TOKEN_COUNTS = [0, 8_000, 64_000, 128_000]\n",
    "HUMANEVAL_PROBLEM_INDICES = [100, 13, 152, 157, 27, 40, 55, 66, 72, 74]\n",
    "INSERT_WHERES = [ \"first half\", \"second half\"]\n",
    "\n",
    "benchmark = datasets.Dataset.from_list(\n",
    "    [build_prompt(*x) for x in itertools.product(APPROXIMATE_TOKEN_COUNTS, HUMANEVAL_PROBLEM_INDICES, INSERT_WHERES)])\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  7.40ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17118502"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.to_json(\"benchmark.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\ndef make_a_pile(n):\\n    \"\"\"\\n    Given a positive integer n, you have to make a pile of n levels of stones.\\n    The first level has n stones.\\n    The number of stones in the next level is:\\n        - the next odd number if n is odd.\\n        - the next even number if n is even.\\n    Return the number of stones in each level in a list, where element at index\\n    i represents the number of stones in the level (i+1).\\n\\n    Examples:\\n    >>> make_a_pile(3)\\n    [3, 5, 7]\\n    \"\"\"\\n    return [n + 2*i for i in range(n)]\\n',\n",
       " 'target_function': '\\ndef make_a_pile(n):\\n    \"\"\"\\n    Given a positive integer n, you have to make a pile of n levels of stones.\\n    The first level has n stones.\\n    The number of stones in the next level is:\\n        - the next odd number if n is odd.\\n        - the next even number if n is even.\\n    Return the number of stones in each level in a list, where element at index\\n    i represents the number of stones in the level (i+1).\\n\\n    Examples:\\n    >>> make_a_pile(3)\\n    [3, 5, 7]\\n    \"\"\"\\n    return [n + 2*i for i in range(n)]\\n',\n",
       " 'humaneval_task_id': 'HumanEval/100',\n",
       " 'task_id': 'LongBench_HumanEval/100_0_first half',\n",
       " 'approx_token_count': 0,\n",
       " 'target_function_name': 'make_a_pile'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
